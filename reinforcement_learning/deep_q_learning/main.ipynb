{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-rl2\n",
    "!pip install gym[atari]\n",
    "!pip install atari-py\n",
    "!pip install tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms /content/drive/MyDrive/dqn/roms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import register\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='Breakout-v4',\n",
    "    entry_point='gym.envs.atari:AtariEnv',\n",
    "    kwargs={'game': 'breakout', 'obs_type': 'image', 'frameskip': 1},\n",
    "    max_episode_steps=10000,\n",
    "    nondeterministic=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Breakout Environment\n",
    "env = gym.make('Breakout-v4')\n",
    "np.random.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# NN Model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "# Print Model Summary For Visuals\n",
    "print(model.summary())\n",
    "\n",
    "# Compile\n",
    "memory = SequentialMemory(limit=1000000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "weights = None\n",
    "\n",
    "class CustomModelCheckpoint(Callback):\n",
    "    \"\"\"\n",
    "    Custom Model Checkpoint: callbacks\n",
    "\n",
    "    Created to handle error happening with this line:\n",
    "\n",
    "    dqn.fit(env, callbacks=callbacks_list, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "    AttributeError: 'DQNAgent' object has no attribute 'distribute_strategy'\n",
    "\n",
    "    Saves the models weights without having to use distribute_strategy\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath, monitor='val_loss', save_best_only=False):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_value = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is None:\n",
    "            return\n",
    "        if self.best_value is None or current_value > self.best_value:\n",
    "            self.best_value = current_value\n",
    "            self.model.save_weights(self.filepath, overwrite=True)\n",
    "            print(f\"Saved model weights at {self.filepath} - {self.monitor}: {current_value}\")\n",
    "\n",
    "# Usage:\n",
    "custom_checkpoint = CustomModelCheckpoint(filepath='/content/drive/MyDrive/policy.h5', monitor='episode_reward', save_best_only=True)\n",
    "callbacks_list = [custom_checkpoint]\n",
    "\n",
    "dqn.fit(env, callbacks=callbacks_list, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# Save the final policy network\n",
    "dqn.save_weights('policy_final.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filename = f'/content/drive/MyDrive/policy.h5'\n",
    "if weights:\n",
    "    weights_filename = weights\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
